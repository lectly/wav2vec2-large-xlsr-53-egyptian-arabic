{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccK0Jc-x3zj7",
        "outputId": "94690c43-0b7c-4016-95cc-749e402cb831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 28.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 50.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n",
            "\u001b[K     |████████████████████████████████| 362 kB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.3 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 60.5 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 854 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "id": "ccK0Jc-x3zj7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6ecbd546"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "from datasets import load_metric\n",
        "from ast import literal_eval"
      ],
      "id": "6ecbd546"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqbdM67ZK1FG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "YqbdM67ZK1FG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6de8e218"
      },
      "outputs": [],
      "source": [
        "language_code = 'ar'\n",
        "language_name = 'arabic'\n",
        "base_model = \"facebook/wav2vec2-large-xlsr-53\"\n",
        "\n",
        "data_dir = \"./drive/My Drive/Senior Project/Resources/data\"\n",
        "bucket_name = \"hearsome-sagemaker-datasets\"\n",
        "\n",
        "output_models_dir = f\"./workspace/output_models/{language_code}/wav2vec2-large-xlsr-{language_name}-demo\"\n",
        "new_output_models_dir = f\"./drive/My Drive/Senior Project/Resources/workspace/output_models/{language_code}/wav2vec2-large-xlsr-{language_name}\""
      ],
      "id": "6de8e218"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6abbe1c"
      },
      "outputs": [],
      "source": [
        "train = pd.read_pickle(f\"{data_dir}/train.pbz2\", compression='bz2')\n",
        "dev = pd.read_pickle(f\"{data_dir}/dev.pbz2\", compression='bz2')"
      ],
      "id": "b6abbe1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d115c45"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ],
      "id": "0d115c45"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86d78b05"
      },
      "outputs": [],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(new_output_models_dir)"
      ],
      "id": "86d78b05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "621e64b9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MGB3Dataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.data = df[[\"input_values\", \"input_length\", \"labels\"]].to_dict('records')\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]"
      ],
      "id": "621e64b9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e2a5f0a"
      },
      "outputs": [],
      "source": [
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "        \n",
        "    def __init__(self, processor: Wav2Vec2Processor, padding: Union[bool, str]):\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "id": "4e2a5f0a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66d6aa20"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "id": "66d6aa20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zJbkfHR4osJ"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ],
      "id": "3zJbkfHR4osJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a879d0b6"
      },
      "outputs": [],
      "source": [
        "wer_metric = load_metric(\"wer\")"
      ],
      "id": "a879d0b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ccbecc"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "id": "18ccbecc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e7e2d96"
      },
      "outputs": [],
      "source": [
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-xls-r-300m\", \n",
        "    attention_dropout=0.0,\n",
        "    hidden_dropout=0.0,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.0,\n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer),\n",
        ")"
      ],
      "id": "4e7e2d96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f2e3c18"
      },
      "outputs": [],
      "source": [
        "model.freeze_feature_encoder()"
      ],
      "id": "6f2e3c18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ad26efe"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "  output_dir=new_output_models_dir,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=30,\n",
        "  gradient_checkpointing=True,\n",
        "  fp16=True,\n",
        "  save_steps=600,\n",
        "  eval_steps=400,\n",
        "  logging_steps=400,\n",
        "  learning_rate=3e-4,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        ")"
      ],
      "id": "1ad26efe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e022fe05"
      },
      "outputs": [],
      "source": [
        "train_dataset = MGB3Dataset(train)\n",
        "dev_dataset = MGB3Dataset(dev)"
      ],
      "id": "e022fe05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "349620fd"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "id": "349620fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13451397"
      },
      "outputs": [],
      "source": [
        "trainer.train(\n",
        "    resume_from_checkpoint=True,\n",
        ")"
      ],
      "id": "13451397"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzOETjPUvYpb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_metric\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MGB3Dataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.data = df[[\"input_values\", \"input_length\", \"labels\"]].to_dict('records')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "\n",
        "test = pd.read_pickle(f\".{data_dir}/test.pbz2\", compression='bz2')\n",
        "test_dataset = MGB3Dataset(test)\n",
        "\n",
        "\n",
        "wer = load_metric(\"wer\")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"Zaid/wav2vec2-large-xlsr-53-arabic-egyptian\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"Zaid/wav2vec2-large-xlsr-53-arabic-egyptian\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "print(\"WER: {:2f}\".format(100 * wer.compute(predictions=test_dataset[\"pred_strings\"],\n",
        "                                            references=test_dataset[\"sentence\"])))"
      ],
      "id": "kzOETjPUvYpb"
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Train.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}